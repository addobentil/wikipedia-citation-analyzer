{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b354b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================[ WIKIPEDIA CITATION ANALYZER - FINAL WORKING VERSION ]=====================\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "from google.colab import files\n",
    "from google.colab import userdata\n",
    "from typing import List, Dict\n",
    "\n",
    "# =====================[ ASCII ART HEADER ]=====================\n",
    "def display_header():\n",
    "    print(r\"\"\"\n",
    "+-----------------------------------------------+\n",
    "|  üîç Wikipedia Citation Analyzer (Bot Edition) |\n",
    "+-----------------------------------------------+\n",
    "|  ‚Ä¢ Bot-authenticated API access              |\n",
    "|  ‚Ä¢ Scans for incomplete Cite web templates   |\n",
    "|  ‚Ä¢ Processes multiple articles               |\n",
    "|  ‚Ä¢ Created by Addobentil@CitationAnalyzerBot |\n",
    "+-----------------------------------------------+\n",
    "\"\"\")\n",
    "\n",
    "# =====================[ CONFIGURATION ]=====================\n",
    "class Config:\n",
    "    API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    USER_AGENT = \"CitationAnalyzerBot/1.0\"\n",
    "    BOT_USERNAME = userdata.get('WIKI_BOT_USERNAME')\n",
    "    BOT_PASSWORD = userdata.get('WIKI_BOT_PASSWORD')\n",
    "    REQUEST_DELAY = 0.3\n",
    "    MAX_ARTICLES = 2000\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "# =====================[ BOT ANALYZER CLASS ]=====================\n",
    "class WikipediaBotAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': Config.USER_AGENT})\n",
    "        self._login()\n",
    "        self.csv_filename = \"\"\n",
    "        self.results = []\n",
    "\n",
    "    def _login(self):\n",
    "        \"\"\"Authenticate bot account\"\"\"\n",
    "        print(\"\\nüîê Authenticating bot...\")\n",
    "        try:\n",
    "            token_params = {\n",
    "                'action': 'query',\n",
    "                'meta': 'tokens',\n",
    "                'type': 'login',\n",
    "                'format': 'json'\n",
    "            }\n",
    "            token_response = self.session.get(Config.API_URL, params=token_params).json()\n",
    "            login_token = token_response['query']['tokens']['logintoken']\n",
    "\n",
    "            auth_params = {\n",
    "                'action': 'login',\n",
    "                'lgname': Config.BOT_USERNAME,\n",
    "                'lgpassword': Config.BOT_PASSWORD,\n",
    "                'lgtoken': login_token,\n",
    "                'format': 'json'\n",
    "            }\n",
    "            \n",
    "            login_response = self.session.post(Config.API_URL, data=auth_params).json()\n",
    "            if login_response.get('login', {}).get('result') != 'Success':\n",
    "                raise Exception(\"‚ùå Bot login failed. Check credentials.\")\n",
    "            print(\"‚úÖ Authentication successful\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Login error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def api_request(self, params: dict, max_retries=3):\n",
    "        \"\"\"Make authenticated API requests\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(Config.REQUEST_DELAY)\n",
    "                params['format'] = 'json'\n",
    "                response = self.session.post(Config.API_URL, data=params)\n",
    "                \n",
    "                if response.status_code == 429:\n",
    "                    retry_after = int(response.headers.get('Retry-After', 5))\n",
    "                    print(f\"‚è≥ Rate limited. Waiting {retry_after}s...\")\n",
    "                    time.sleep(retry_after)\n",
    "                    continue\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt+1} failed: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep((attempt + 1) * 2)\n",
    "\n",
    "    def get_wikitext_batch(self, titles: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Fetch multiple articles at once\"\"\"\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'content',\n",
    "            'rvslots': 'main',\n",
    "            'titles': '|'.join(titles),\n",
    "            'format': 'json',\n",
    "            'formatversion': '2'\n",
    "        }\n",
    "        \n",
    "        data = self.api_request(params)\n",
    "        results = {}\n",
    "        for page in data.get('query', {}).get('pages', []):\n",
    "            if 'missing' not in page:\n",
    "                results[page['title']] = page['revisions'][0]['slots']['main']['content']\n",
    "        return results\n",
    "\n",
    "    def analyze_citations(self, wikitext: str) -> Dict:\n",
    "        \"\"\"Analyze citations in wikitext\"\"\"\n",
    "        if not wikitext:\n",
    "            return {'total': 0, 'incomplete': [], 'problems': []}\n",
    "\n",
    "        wikitext = wikitext.replace(\"{{!}}\", \"|\").replace(\"{{pipe}}\", \"|\")\n",
    "        pattern = r\"\\{\\{\\s*Cite\\s+web\\s*\\|([^{}]*?(?:\\{[^{}]*\\}[^{}]*)*)\\}\\}\"\n",
    "        citations = re.findall(pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        results = {\n",
    "            'total': len(citations),\n",
    "            'incomplete': [],\n",
    "            'problems': []\n",
    "        }\n",
    "\n",
    "        for citation in citations:\n",
    "            missing = []\n",
    "            text = citation.lower().replace(\" \", \"\")\n",
    "\n",
    "            if not re.search(r\"title\\s*=\", text):\n",
    "                missing.append(\"title\")\n",
    "            if not re.search(r\"url\\s*=\", text):\n",
    "                missing.append(\"url\")\n",
    "\n",
    "            if missing:\n",
    "                results['incomplete'].append(\", \".join(missing))\n",
    "                results['problems'].append({\n",
    "                    'missing': \", \".join(missing),\n",
    "                    'text': f\"{{{{Cite web|{citation}}}}}\"\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        \"\"\"Save results with specified headers\"\"\"\n",
    "        if not self.results:\n",
    "            return \"\"\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.csv_filename = f\"citation_analysis_{timestamp}.csv\"\n",
    "        \n",
    "        with open(self.csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'article title',\n",
    "                'number of total citations',\n",
    "                'number of incomplete citations',\n",
    "                'missing fields',\n",
    "                'problematic citations'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for item in self.results:\n",
    "                problematic_citations = \"\\n\\n\".join(\n",
    "                    [f\"Missing: {prob['missing']}\\n{prob['text']}\" \n",
    "                     for prob in item['problems']]\n",
    "                ) if item['problems'] else \"None\"\n",
    "                \n",
    "                writer.writerow({\n",
    "                    'article title': item['title'],\n",
    "                    'number of total citations': item['total'],\n",
    "                    'number of incomplete citations': len(item['incomplete']),\n",
    "                    'missing fields': \"; \".join(set(item['incomplete']))\n",
    "                })\n",
    "        \n",
    "        return self.csv_filename\n",
    "\n",
    "# =====================[ USER INTERFACE ]=====================\n",
    "def get_article_list():\n",
    "    \"\"\"Get multiple articles from user input\"\"\"\n",
    "    print(\"\\nEnter article titles (one per line, blank line to finish):\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        article = input(\"> \").strip()\n",
    "        if not article:  # Empty line ends input\n",
    "            break\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "def analyze_specific_articles(analyzer):\n",
    "    \"\"\"Analyze specific articles entered by user (no CSV output)\"\"\"\n",
    "    articles = get_article_list()\n",
    "    if not articles:\n",
    "        print(\"\\nNo articles entered. Returning to menu.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüîç Starting analysis of {len(articles)} articles...\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    batch_size = Config.BATCH_SIZE\n",
    "    \n",
    "    for i in range(0, len(articles), batch_size):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        print(f\"\\n=== Processing Articles {i+1}-{min(i+batch_size, len(articles))} ===\")\n",
    "        \n",
    "        try:\n",
    "            wikitexts = analyzer.get_wikitext_batch(batch)\n",
    "            for title, text in wikitexts.items():\n",
    "                print(f\"\\nüìÑ Article: {title}\")\n",
    "                analysis = analyzer.analyze_citations(text)\n",
    "\n",
    "                print(f\"\\nüìä Total citations: {analysis['total']}\")\n",
    "                print(f\"‚ö†Ô∏è Incomplete citations: {len(analysis['incomplete'])}\")\n",
    "\n",
    "                if analysis['problems']:\n",
    "                    print(\"\\nProblematic citations:\")\n",
    "                    for i, problem in enumerate(analysis['problems'], 1):\n",
    "                        print(f\"\\n{i}. Missing: {problem['missing']}\")\n",
    "                        print(problem['text'])\n",
    "                else:\n",
    "                    print(\"‚úÖ All citations are complete!\")\n",
    "                \n",
    "                print(\"\\n\" + \"-\"*50)  # Separator between articles\n",
    "                \n",
    "            total_processed += len(wikitexts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete! Processed {total_processed} articles\")\n",
    "\n",
    "def main():\n",
    "    display_header()\n",
    "    analyzer = WikipediaBotAnalyzer()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"1. Scan random articles using Cite web template (exports CSV)\")\n",
    "        print(\"2. Analyze specific article(s) (screen output only)\")\n",
    "        print(\"3. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nChoose option (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            try:\n",
    "                limit = int(input(f\"\\nHow many articles to scan? (Max {Config.MAX_ARTICLES}): \") or \"100\")\n",
    "                limit = min(limit, Config.MAX_ARTICLES)\n",
    "                \n",
    "                print(f\"\\nüîç Starting analysis of {limit} articles...\")\n",
    "                \n",
    "                # Get pages using template\n",
    "                params = {\n",
    "                    'action': 'query',\n",
    "                    'list': 'embeddedin',\n",
    "                    'eititle': 'Template:Cite_web',\n",
    "                    'eilimit': Config.BATCH_SIZE,\n",
    "                    'format': 'json'\n",
    "                }\n",
    "                \n",
    "                articles = []\n",
    "                while len(articles) < limit:\n",
    "                    batch_start = len(articles) + 1\n",
    "                    batch_end = min(len(articles) + Config.BATCH_SIZE, limit)\n",
    "                    print(f\"\\n=== Processing Articles {batch_start}-{batch_end} ===\")\n",
    "                    print(f\"  Fetching wikitext for {batch_end - batch_start + 1} articles...\")\n",
    "                    \n",
    "                    data = analyzer.api_request(params)\n",
    "                    new_articles = data.get('query', {}).get('embeddedin', [])\n",
    "                    articles.extend(new_articles)\n",
    "                    \n",
    "                    # Process batch immediately after fetching\n",
    "                    if new_articles:\n",
    "                        titles = [a['title'] for a in new_articles]\n",
    "                        print(f\"\\nAnalyzing batch {len(articles)//Config.BATCH_SIZE}/{(limit-1)//Config.BATCH_SIZE + 1}\")\n",
    "                        wikitexts = analyzer.get_wikitext_batch(titles)\n",
    "                        \n",
    "                        for title, text in wikitexts.items():\n",
    "                            print(f\"  Checking: {title[:50]}...\")\n",
    "                            analysis = analyzer.analyze_citations(text)\n",
    "                            \n",
    "                            if analysis['incomplete']:\n",
    "                                analyzer.results.append({\n",
    "                                    'title': title,\n",
    "                                    **analysis\n",
    "                                })\n",
    "                    \n",
    "                    if 'continue' not in data or len(articles) >= limit:\n",
    "                        break\n",
    "                    params.update(data['continue'])\n",
    "                \n",
    "                total_incomplete = sum(len(item['incomplete']) for item in analyzer.results)\n",
    "                print(f\"\\n‚úÖ Scan complete! Found {len(analyzer.results)} articles with {total_incomplete} incomplete citations\")\n",
    "                \n",
    "                # Save and show results\n",
    "                if analyzer.results:\n",
    "                    print(\"\\nüíæ Saving results...\")\n",
    "                    filename = analyzer.save_to_csv()\n",
    "                    print(f\"\\nüìè Results saved to: {filename}\")\n",
    "                    print(\"\\nTo download:\")\n",
    "                    print(f\"1. Click the Colab folder iconüìÅ on the left-sidebar\")\n",
    "                    print(f\"2. Right click on the file and select download.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error during scan: {str(e)}\")\n",
    "                \n",
    "        elif choice == \"2\":\n",
    "            analyzer.results = []  # Clear previous results\n",
    "            analyze_specific_articles(analyzer)\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            print(\"\\nThank you for using Wikipedia Citation Analyzer!\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
